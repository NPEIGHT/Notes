## Maximum Likelihood Estimation (MLE)
* 估计模型参数，使得极大似然函数取得最大值
$$
\mathrm{L}\left(\theta | x_{1}, \ldots, x_{n}\right)=f_{\theta}\left(x_{1}, \ldots, x_{n}\right)
$$
* 对于一个模型，有一些观测数据$y_1,y_2$，需要估计位置的模型参数$\theta$，使用MLE，即令$P(y_1,y_2;\theta)$最大
* 对数形式：假设每个观测值是相互独立的，那么似然函数可以写成概率密度的乘积，直接计算最大值不方便，利用对数来简化计算

## Note: Discriminative Training of Kalman Filters
十五年前，斯坦福大佬们就在用DL来水这类文章了……开玩笑，其实只是用了迭代优化的思路——用少量高精度的测量数据，快速获得模型参数的最佳值  

训练R和Q主要有以下几种方法：
1. 极大似然估计（MLE）：根据所有的观测，计算使测量似然函数最大化的R和Q，需要所有的状态向量——观测矩阵是单位阵，同时观测噪声可以忽略，实际无法实现，因为很多状态是不能观测的
2. 最小化预测残差：在训练EKF时最小化预测误差，取得使训练测量值和预测值差距最小的R和Q，由于误差函数和R、Q之间的关系通过预测均值体现，表现形式比较复杂，但是这种方法评估了滤波器的实际性能
3. 最大化预测似然函数：同时考虑预测协方差，反映状态估计的实际分布
4. 最大化测量似然函数：利用类似思想，在不借助额外数据的情况下，通过测量和输入调参，最大化测量似然函数
5. 平滑后优化：最小化平滑估计值和训练测量值的残差，但是平滑需要对所有测量值进行处理

测试结果：